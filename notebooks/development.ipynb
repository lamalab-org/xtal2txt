{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load json\n",
    "def load_json(file):\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json('/home/so87pot/n0w0f/structllm_data/qmof_filtered.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymatgen.core.structure import Structure\n",
    "\n",
    "def dict_to_structure(structure_dict):\n",
    "    return Structure.from_dict(structure_dict).to(fmt='cif')\n",
    "\n",
    "def filter_data(data,number_of_atoms):\n",
    "    filtered_data = []\n",
    "    for entry in data:\n",
    "        if all(key in entry['data'] for key in ['natoms', 'pld', 'lcd', 'density', 'EgPBE', 'volume']):\n",
    "            if entry['data']['natoms']['value'] < number_of_atoms:\n",
    "                filtered_entry = {\n",
    "                    'id': entry['id'],\n",
    "                    'structure': dict_to_structure(entry['structure']),\n",
    "                    'natoms': entry['data']['natoms']['value'],\n",
    "                    'pld': entry['data']['pld']['value'],\n",
    "                    'lcd': entry['data']['lcd']['value'],\n",
    "                    'density': entry['data']['density']['value'],\n",
    "                    'EgPBE': entry['data']['EgPBE']['value'],\n",
    "                    'volume': entry['data']['volume']['value'],\n",
    "                }\n",
    "                filtered_data.append(filtered_entry)\n",
    "    return filtered_data\n",
    "\n",
    "# code  to call filter data and save json\n",
    "data = load_json('/home/so87pot/n0w0f/structllm/src/structllm/dataprep/qmof_dataset/screened_mofs.json')\n",
    "\n",
    "filtered_data = filter_data(data,100)\n",
    "with open('filtered_data.json', 'w') as f:\n",
    "    json.dump(filtered_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '6196c6e7a6be6ad338993951',\n",
       " 'structure': \"# generated using pymatgen\\ndata_Ba2CuH14(C3O8)2\\n_symmetry_space_group_name_H-M   'P 1'\\n_cell_length_a   6.94195231\\n_cell_length_b   7.17878940\\n_cell_length_c   8.79165165\\n_cell_angle_alpha   82.25599710\\n_cell_angle_beta   71.35701702\\n_cell_angle_gamma   81.58285545\\n_symmetry_Int_Tables_number   1\\n_chemical_formula_structural   Ba2CuH14(C3O8)2\\n_chemical_formula_sum   'Ba2 Cu1 H14 C6 O16'\\n_cell_volume   408.85747126\\n_cell_formula_units_Z   1\\nloop_\\n _symmetry_equiv_pos_site_id\\n _symmetry_equiv_pos_as_xyz\\n  1  'x, y, z'\\nloop_\\n _atom_site_type_symbol\\n _atom_site_label\\n _atom_site_symmetry_multiplicity\\n _atom_site_fract_x\\n _atom_site_fract_y\\n _atom_site_fract_z\\n _atom_site_occupancy\\n  Ba  Ba0  1  0.09830491  0.73435257  0.35340987  1\\n  Ba  Ba1  1  0.90169454  0.26564683  0.64659097  1\\n  Cu  Cu2  1  0.00000517  0.99999776  0.99999986  1\\n  H  H3  1  0.59629623  0.90436682  0.07226429  1\\n  H  H4  1  0.40370491  0.09563116  0.92773741  1\\n  H  H5  1  0.00460535  0.36603387  0.08997702  1\\n  H  H6  1  0.99539477  0.63395988  0.91001972  1\\n  H  H7  1  0.45740440  0.39774234  0.12471460  1\\n  H  H8  1  0.54259522  0.60225564  0.87527952  1\\n  H  H9  1  0.34667918  0.05719876  0.42535163  1\\n  H  H10  1  0.65332314  0.94280205  0.57465515  1\\n  H  H11  1  0.30121594  0.92728771  0.59164359  1\\n  H  H12  1  0.69878199  0.07271169  0.40836218  1\\n  H  H13  1  0.38406320  0.34763759  0.41905977  1\\n  H  H14  1  0.61593675  0.65235756  0.58094141  1\\n  H  H15  1  0.35028880  0.40454035  0.59270859  1\\n  H  H16  1  0.64970812  0.59546754  0.40729081  1\\n  C  C17  1  0.65178813  0.80750039  0.16207635  1\\n  C  C18  1  0.34821436  0.19249901  0.83792485  1\\n  C  C19  1  0.04497797  0.27531563  0.19007171  1\\n  C  C20  1  0.95502300  0.72468376  0.80992647  1\\n  C  C21  1  0.57703131  0.28113492  0.13766959  1\\n  C  C22  1  0.42296661  0.71886448  0.86232822  1\\n  O  O23  1  0.83700339  0.81494706  0.15897972  1\\n  O  O24  1  0.16299910  0.18505234  0.84102035  1\\n  O  O25  1  0.53521993  0.69684834  0.25823823  1\\n  O  O26  1  0.46478054  0.30315530  0.74176757  1\\n  O  O27  1  0.06417235  0.35074004  0.30403184  1\\n  O  O28  1  0.93581917  0.64926219  0.69596727  1\\n  O  O29  1  0.07041927  0.09574826  0.17915093  1\\n  O  O30  1  0.92959099  0.90424973  0.82084733  1\\n  O  O31  1  0.71897665  0.24145801  0.01129704  1\\n  O  O32  1  0.28102601  0.75853997  0.98870201  1\\n  O  O33  1  0.55542234  0.20130160  0.28060418  1\\n  O  O34  1  0.44457238  0.79869921  0.71939428  1\\n  O  O35  1  0.24143604  0.98685352  0.50576925  1\\n  O  O36  1  0.75856307  0.01314871  0.49423591  1\\n  O  O37  1  0.28603665  0.42259145  0.50474150  1\\n  O  O38  1  0.71396263  0.57740937  0.49525694  1\\n\",\n",
       " 'natoms': 39.0,\n",
       " 'pld': 0.68822,\n",
       " 'lcd': 1.3548,\n",
       " 'density': 2.76325,\n",
       " 'EgPBE': 0.632527,\n",
       " 'volume': 408.857}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 23:00:40.450977: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-23 23:00:40.451523: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-23 23:00:40.582877: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/so87pot/miniconda3/envs/xtal2/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading file /home/so87pot/miniconda3/envs/xtal2/lib/python3.9/site-packages/robocrys/condense/mineral_db.json.gz: 0it [00:00, ?it/s]#####6| 173/180 [00:00<00:00, 626.71it/s]\n",
      "Decoding objects from /home/so87pot/miniconda3/envs/xtal2/lib/python3.9/site-packages/robocrys/condense/mineral_db.json.gz: 100%|##########| 180/180 [00:00<00:00, 566.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from xtal2txt.core import TextRep\n",
    "transformations = [\n",
    "    #(\"permute_structure\", {\"seed\": 42}),\n",
    "     (\"translate_single_atom\", {\"seed\": 42}),\n",
    "     (\"perturb_structure\", {\"seed\": 42, \"max_distance\":0.1}),\n",
    "     (\"translate_structure\", {\"seed\": 42, \"vector\": [0.1,0.1,0.1], \"frac_coords\": True}),\n",
    "    ]\n",
    "\n",
    "from pymatgen.core import Structure\n",
    "from_file  = \"/home/so87pot/n0w0f/xtal2txt/tests/data/InCuS2_p1.cif\"\n",
    "structure = Structure.from_file(str(from_file), \"cif\")\n",
    "structure\n",
    "\n",
    "text_rep = TextRep.from_input(structure, transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updated_zmatrix_rep(zmatrix, decimal_places=1):\n",
    "    lines = zmatrix.split('\\n')\n",
    "    main_part = []\n",
    "    variables_part = []\n",
    "\n",
    "    # Determine the main part and the variables part of the Z-matrix\n",
    "    for line in lines:\n",
    "        if '=' in line:\n",
    "            variables_part.append(line)\n",
    "        else:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                main_part.append(line)\n",
    "\n",
    "    # Extract variables from the variables part\n",
    "    variable_dict = {}\n",
    "    for var_line in variables_part:\n",
    "        var, value = var_line.split('=')\n",
    "        if var.startswith('B'):\n",
    "            rounded_value = round(float(value.strip()), decimal_places)\n",
    "        else:\n",
    "            rounded_value = int(round(float(value.strip())))\n",
    "        variable_dict[var] = f\"{rounded_value}\" if var.startswith(('A', 'D')) else f\"{rounded_value:.{decimal_places}f}\"\n",
    "\n",
    "    # Replace variables in the main part\n",
    "    replaced_lines = []\n",
    "    for line in main_part:\n",
    "        parts = line.split()\n",
    "        atom = parts[0]\n",
    "        replaced_line = line\n",
    "        for i in range(1, len(parts)):\n",
    "            var = parts[i]\n",
    "            if var in variable_dict:\n",
    "                replaced_line = replaced_line.replace(var, variable_dict[var])\n",
    "        replaced_lines.append(replaced_line)\n",
    "\n",
    "    return '\\n'.join(replaced_lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "def get_distance(self, i: int, j: int) -> float:\n",
    "        \"\"\"Get distance between site i and j.\n",
    "\n",
    "        Args:\n",
    "            i (int): 1st site index\n",
    "            j (int): 2nd site index\n",
    "\n",
    "        Returns:\n",
    "            Distance between the two sites.\n",
    "        \"\"\"\n",
    "        return self[i].distance(self[j])\n",
    "\n",
    "def _find_nn_pos_before_site(self, site_idx):\n",
    "        \"\"\"Returns index of nearest neighbor atoms.\"\"\"\n",
    "        all_dist = [(self.get_distance(site_idx, idx), idx) for idx in range(site_idx)]\n",
    "        all_dist = sorted(all_dist, key=lambda x: x[0])\n",
    "        return [d[1] for d in all_dist]\n",
    "\n",
    "def get_zmatrix(self, molecule, decimal_places=6):\n",
    "    \"\"\"Returns a z-matrix representation of the molecule.\"\"\"\n",
    "    output = []\n",
    "    output_var = []\n",
    "    for idx, site in enumerate(molecule):\n",
    "        if idx == 0:\n",
    "            output.append(f\"{site.specie}\")\n",
    "        else:\n",
    "            nn = self._find_nn_pos_before_site(molecule, idx)\n",
    "            bond_length = molecule.get_distance(idx, nn[0])\n",
    "            bond_length_str = f\"{bond_length:.{decimal_places}f}\"\n",
    "            bond_rep = f\"{nn[0] + 1} {bond_length_str}\"\n",
    "            bond_var = f\"B{idx}={bond_length_str}\"\n",
    "            if idx == 1:\n",
    "                output.append(f\"{molecule[idx].specie} {bond_rep}\")\n",
    "            elif idx == 2:\n",
    "                angle = molecule.get_angle(idx, nn[0], nn[1])\n",
    "                angle_str = f\"{angle:.{decimal_places}f}\"\n",
    "                output.append(f\"{molecule[idx].specie} {bond_rep} {nn[1] + 1} {angle_str}\")\n",
    "                output_var.append(f\"A{idx}={angle_str}\")\n",
    "            else:\n",
    "                angle = molecule.get_angle(idx, nn[0], nn[1])\n",
    "                angle_str = f\"{angle:.{decimal_places}f}\"\n",
    "                dihedral = molecule.get_dihedral(idx, nn[0], nn[1], nn[2])\n",
    "                dihedral_str = f\"{dihedral:.{decimal_places}f}\"\n",
    "                output.append(f\"{molecule[idx].specie} {bond_rep} {nn[1] + 1} {angle_str} {nn[2] + 1} {dihedral_str}\")\n",
    "                output_var.extend([f\"A{idx}={angle_str}\", f\"D{idx}={dihedral_str}\"])\n",
    "            output_var.append(bond_var)\n",
    "    return \"\\n\".join(output) + \"\\n\\n\" + \"\\n\".join(output_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "import re\n",
    "class NumTokenizer:\n",
    "    \"\"\"Tokenize numbers as implemented in Regression Transformer.\n",
    "        https://www.nature.com/articles/s42256-023-00639-z\"\"\"\n",
    "        \n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Tokenizer for numbers.\"\"\"\n",
    "        self.regex = re.compile(r\"(\\+|-)?(\\d+)(\\.)?(\\d+)?\\s*\")\n",
    "\n",
    "    import re\n",
    "\n",
    "    def num_matcher(self, text: str) -> str:\n",
    "        \"\"\"Extract numbers from a sentence and replace them with tokens.\"\"\"\n",
    "        matches = re.findall(r'\\b\\d+(?:\\.\\d+)?\\b', text)  # This regex captures both whole numbers and decimal numbers\n",
    "        for match in matches:\n",
    "            tokens = self.tokenize(match)\n",
    "            replacement = ' '.join(tokens)\n",
    "            text = re.sub(r'\\b' + re.escape(match) + r'\\b', replacement, text, count=1)  # replace only the first occurrence\n",
    "        return text\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenization of a property.\n",
    "\n",
    "        Args:\n",
    "            text: number as string to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            extracted tokens.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        matched = self.regex.match(text)\n",
    "        if matched:\n",
    "            sign, units, dot, decimals = matched.groups()\n",
    "            tokens = []\n",
    "            if sign:\n",
    "                tokens += [f\"_{sign}_\"]\n",
    "            tokens += [\n",
    "                f\"_{number}_{position}_\" for position, number in enumerate(units[::-1])\n",
    "            ][::-1]\n",
    "            if dot:\n",
    "                tokens += [f\"_{dot}_\"]\n",
    "            if decimals:\n",
    "                tokens += [\n",
    "                    f\"_{number}_-{position}_\"\n",
    "                    for position, number in enumerate(decimals, 1)\n",
    "                ]\n",
    "        return tokens\n",
    "    \n",
    "    @staticmethod\n",
    "    def floating_tokens_to_float(token_ids: List[str]) -> float:\n",
    "        \"\"\"Converts tokens representing a float value into a float.\n",
    "        NOTE: Expects that non-floating tokens are strippped off\n",
    "\n",
    "        Args:\n",
    "            token_ids: List of tokens, each representing a float.\n",
    "                E.g.: ['_0_0_', '_._', '_9_-1_', '_3_-2_', '_1_-3_']\n",
    "\n",
    "        Returns:\n",
    "            float: Float representation for the list of tokens.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            float_string = \"\".join([token.split(\"_\")[1] for token in token_ids])\n",
    "            float_value = float(float_string)\n",
    "        except ValueError:\n",
    "            float_value = -1\n",
    "        return float_value\n",
    "\n",
    "    \n",
    "    def convert_tokens_to_string(self, tokens: List[str]) -> str:\n",
    "        \"\"\"Converts tokens to string.\n",
    "\n",
    "        Args:\n",
    "            tokens: List of tokens.\n",
    "\n",
    "        Returns:\n",
    "            str: String representation of the tokens.\n",
    "        \"\"\"\n",
    "        return \"\".join([token.split(\"_\")[1] for token in tokens])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def num_matcher(text: str) -> str:\n",
    "    \"\"\"Extract numbers from a sentence and replace them with tokens.\"\"\"\n",
    "    matches = re.findall(r'(\\d+\\.\\d+|\\d+)', text)  # This regex captures both whole numbers and decimal numbers\n",
    "    print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '1', '5.52', '5.52']\n"
     ]
    }
   ],
   "source": [
    "num_matcher(\"data_InCuS2\\n_symmetry_space_group_name_H-M   P1\\n_cell_length_a   5.52\\n_cell_length_b   5.52\\n_cell_length_c  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xtal2txt.core import TextRep\n",
    "transformations = [\n",
    "    #(\"permute_structure\", {\"seed\": 42}),\n",
    "    #  (\"translate_single_atom\", {\"seed\": 42}),\n",
    "    #  (\"perturb_structure\", {\"seed\": 42, \"max_distance\":0.1}),\n",
    "    #  (\"translate_structure\", {\"seed\": 42, \"vector\": [0.1,0.1,0.1], \"frac_coords\": True}),\n",
    "    ]\n",
    "\n",
    "from pymatgen.core import Structure\n",
    "from_file  = \"/home/so87pot/n0w0f/xtal2txt/tests/data/SrTiO3_p1.cif\"\n",
    "structure = Structure.from_file(str(from_file), \"cif\")\n",
    "structure\n",
    "\n",
    "text_rep = TextRep.from_input(structure, transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Open the text file and read the lines\n",
    "with open('/home/so87pot/n0w0f/xtal2txt/src/xtal2txt/vocabs/cif_vocab_rt.txt', 'r') as f:\n",
    "    lines = [line.strip() for line in f]\n",
    "\n",
    "# Create a dictionary where the keys are the lines and the values are sequential numbers\n",
    "data = {line: i for i, line in enumerate(lines, start=164)}\n",
    "\n",
    "# Write the dictionary to a JSON file\n",
    "with open('/home/so87pot/n0w0f/xtal2txt/src/xtal2txt/vocabs/cif_vocab_rt.json', 'w') as f:\n",
    "    json.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transformations = None\n",
    "cif = TextRep.from_input(structure, transformations).get_cif_string(format=\"p1\")\n",
    "slice = TextRep.from_input(structure, transformations).get_slice()\n",
    "crystal = TextRep.from_input(structure, transformations).get_crystal_llm_rep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9 3.9 3.9\\n90 90 90\\nSr2+\\n0.00 0.00 0.00\\nTi4+\\n0.50 0.50 0.50\\nO2-\\n0.50 0.00 0.50\\nO2-\\n0.50 0.50 0.00\\nO2-\\n0.00 0.50 0.50'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crystal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xtal2txt.tokenizer import CifTokenizer, CrysllmTokenizer, SliceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciftokenizer = CifTokenizer(special_num_token=True,model_max_length=512, truncation=False, padding=False)\n",
    "\n",
    "slicetokenizer = SliceTokenizer(special_num_token=True,model_max_length=512, truncation=False, padding=False)\n",
    "\n",
    "crystaltokenizer = CrysllmTokenizer(special_num_token=True,model_max_length=512, truncation=False, padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"data_N2\\n_symmetry_space_group_name_H-M   'P 1'\\n_cell_length_a   5.605\\n_cell_length_b   5.605\\n_cell_length_c   5.605\\n_cell_angle_alpha   90.0\\n_cell_angle_beta   90.0\\n_cell_angle_gamma   90.0\\n_symmetry_Int_Tables_number   1\\n_chemical_formula_structural   N2\\n_chemical_formula_sum   N4\\n_cell_volume   176.125\\n_cell_formula_units_Z   2\\nloop_\\n _symmetry_equiv_pos_site_id\\n _symmetry_equiv_pos_as_xyz\\n  1  'x, y, z'\\nloop_\\n _atom_type_symbol\\n _atom_type_oxidation_number\\n  N0+  0.0\\nloop_\\n _atom_site_type_symbol\\n _atom_site_label\\n _atom_site_symmetry_multiplicity\\n _atom_site_fract_x\\n _atom_site_fract_y\\n _atom_site_fract_z\\n _atom_site_occupancy\\n  N0+  N0  1  0.477  0.977  0.523  1.0\\n  N0+  N1  1  0.977  0.523  0.477  1.0\\n  N0+  N2  1  0.023  0.023  0.023  1.0\\n  N0+  N3  1  0.523  0.477  0.977  1.0\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = crystaltokenizer.tokenize(crystal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_3_0_',\n",
       " '_._',\n",
       " '_9_-1_',\n",
       " ' ',\n",
       " '_3_0_',\n",
       " '_._',\n",
       " '_9_-1_',\n",
       " ' ',\n",
       " '_3_0_',\n",
       " '_._',\n",
       " '_9_-1_',\n",
       " '\\n',\n",
       " '_9_1_',\n",
       " '_0_0_',\n",
       " ' ',\n",
       " '_9_1_',\n",
       " '_0_0_',\n",
       " ' ',\n",
       " '_9_1_',\n",
       " '_0_0_',\n",
       " '\\n',\n",
       " 'Sr',\n",
       " '_2_0_',\n",
       " '+',\n",
       " '\\n',\n",
       " '_0_0_',\n",
       " '_._',\n",
       " '_0_-1_',\n",
       " '_0_-2_',\n",
       " ' ',\n",
       " '_0_0_',\n",
       " '_._',\n",
       " '_0_-1_',\n",
       " '_0_-2_',\n",
       " ' ',\n",
       " '_0_0_',\n",
       " '_._',\n",
       " '_0_-1_',\n",
       " '_0_-2_',\n",
       " '\\n',\n",
       " 'Ti',\n",
       " '_4_0_',\n",
       " '+',\n",
       " '\\n',\n",
       " '_0_0_',\n",
       " '_._',\n",
       " '_5_-1_',\n",
       " '_0_-2_',\n",
       " ' ',\n",
       " '_0_0_',\n",
       " '_._',\n",
       " '_5_-1_',\n",
       " '_0_-2_',\n",
       " ' ',\n",
       " '_0_0_',\n",
       " '_._',\n",
       " '_5_-1_',\n",
       " '_0_-2_',\n",
       " '\\n',\n",
       " 'O',\n",
       " '_2_0_',\n",
       " '-',\n",
       " '\\n',\n",
       " '_0_0_',\n",
       " '_._',\n",
       " '_5_-1_',\n",
       " '_0_-2_',\n",
       " ' ',\n",
       " '_0_0_',\n",
       " '_._',\n",
       " '_0_-1_',\n",
       " '_0_-2_',\n",
       " ' ',\n",
       " '_0_0_',\n",
       " '_._',\n",
       " '_5_-1_',\n",
       " '_0_-2_',\n",
       " '\\n',\n",
       " 'O',\n",
       " '_2_0_',\n",
       " '-',\n",
       " '\\n',\n",
       " '_0_0_',\n",
       " '_._',\n",
       " '_5_-1_',\n",
       " '_0_-2_',\n",
       " ' ',\n",
       " '_0_0_',\n",
       " '_._',\n",
       " '_5_-1_',\n",
       " '_0_-2_',\n",
       " ' ',\n",
       " '_0_0_',\n",
       " '_._',\n",
       " '_0_-1_',\n",
       " '_0_-2_',\n",
       " '\\n',\n",
       " 'O',\n",
       " '_2_0_',\n",
       " '-',\n",
       " '\\n',\n",
       " '_0_0_',\n",
       " '_._',\n",
       " '_0_-1_',\n",
       " '_0_-2_',\n",
       " ' ',\n",
       " '_0_0_',\n",
       " '_._',\n",
       " '_5_-1_',\n",
       " '_0_-2_',\n",
       " ' ',\n",
       " '_0_0_',\n",
       " '_._',\n",
       " '_5_-1_',\n",
       " '_0_-2_']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xtal2txt.tokenizer import NumTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokenizer = NumTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P1\\n_cell_length_a   _5_0__.__5_-1__2_-2_\\n_cell_length_b   _5_0__.__5_-1__2_-2_\\n_cell_length_c   _6_0__.__7_-1__9_-2__6_-3_\\n_cell_angle_alpha'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokenizer.num_matcher(\"P1\\n_cell_length_a   5.52\\n_cell_length_b   5.52\\n_cell_length_c   6.796\\n_cell_angle_alpha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokenizer.tokenize(\"P1\\n_cell_length_a   _5_0_ _._ _5_-1_ _2_-2_\\n_cell_length_b   _5_0_ _._ _5_-1_ _2_-2_\\n_cell_length_c   _6_0_ _._ _7_-1_ _9_-2_ _6_-3_\\n_cell_angle_alpha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'InCuS2'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokenizer.num_matcher(\"InCuS2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def num_matcher_( text: str) -> str:\n",
    "        \"\"\"Extract numbers from a sentence and replace them with tokens.\"\"\"\n",
    "        matches = re.findall(r'(\\d+\\.\\d+|\\d+)', text)  # This regex captures both whole numbers and decimal numbers\n",
    "        print(matches)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2']\n"
     ]
    }
   ],
   "source": [
    "num_matcher_(\"InCuS2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "\n",
    "THIS_DIR = os.path.dirname(\"/home/so87pot/n0w0f/xtal2txt/src/xtal2txt/\")\n",
    "\n",
    "SLICE_VOCAB = os.path.join(THIS_DIR, \"vocabs\", \"slice_vocab.txt\")\n",
    "SLICE_RT_VOCAB = os.path.join(THIS_DIR, \"vocabs\", \"slice_vocab_rt.txt\")\n",
    "\n",
    "COMPOSITION_VOCAB = os.path.join(THIS_DIR, \"vocabs\", \"composition_vocab.txt\")\n",
    "\n",
    "CIF_VOCAB = os.path.join(THIS_DIR, \"vocabs\", \"cif_vocab.json\")\n",
    "CIF_RT_VOCAB = os.path.join(THIS_DIR, \"vocabs\", \"cif_vocab_rt.json\")\n",
    "\n",
    "CRYSTAL_LLM_VOCAB = os.path.join(THIS_DIR, \"vocabs\", \"crystal_llm_vocab.json\")\n",
    "CRYSTAL_LLM_RT_VOCAB = os.path.join(THIS_DIR, \"vocabs\", \"crystal_llm_vocab_rt.json\")\n",
    "\n",
    "\n",
    "ROBOCRYS_VOCAB = os.path.join(THIS_DIR, \"vocabs\", \"robocrys_vocab.json\")\n",
    "\n",
    "from xtal2txt.analysis import (\n",
    "    ANALYSIS_MASK_TOKENS,\n",
    "    CIF_ANALYSIS_DICT,\n",
    "    COMPOSITION_ANALYSIS_DICT,\n",
    "    CRYSTAL_LLM_ANALYSIS_DICT,\n",
    "    SLICE_ANALYSIS_DICT,\n",
    ")\n",
    "\n",
    "\n",
    "class NumTokenizer_():\n",
    "    \"\"\"Tokenize numbers as implemented in Regression Transformer.\n",
    "        https://www.nature.com/articles/s42256-023-00639-z\"\"\"\n",
    "        \n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Tokenizer for numbers.\"\"\"\n",
    "        self.regex = re.compile(r\"(\\+|-)?(\\d+)(\\.)?(\\d+)?\\s*\")\n",
    "\n",
    "    def num_matcher(self, text: str) -> str:\n",
    "        \"\"\"Extract numbers from a sentence and replace them with tokens.\"\"\"\n",
    "        pattern = r\"\\d+(?:\\.\\d+)?\"  # Match any number, whether it is part of a string or not\n",
    "        matches = list(re.finditer(pattern, text))\n",
    "        for match in reversed(matches):\n",
    "            start, end = match.start(), match.end()\n",
    "            tokens = self.tokenize(match.group())\n",
    "            replacement = ''.join(tokens)\n",
    "            text = text[:start] + replacement + text[end:]\n",
    "        return text\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenization of numbers as in RT.\n",
    "         '0.9' -> '_0_0_', '_._', '_9_-1_'\n",
    "\n",
    "        Args:\n",
    "            text: number as string to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            extracted tokens.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        matched = self.regex.match(text)\n",
    "        if matched:\n",
    "            sign, units, dot, decimals = matched.groups()\n",
    "            tokens = []\n",
    "            if sign:\n",
    "                tokens += [f\"_{sign}_\"]\n",
    "            tokens += [\n",
    "                f\"_{number}_{position}_\" for position, number in enumerate(units[::-1])\n",
    "            ][::-1]\n",
    "            if dot:\n",
    "                tokens += [f\"_{dot}_\"]\n",
    "            if decimals:\n",
    "                tokens += [\n",
    "                    f\"_{number}_-{position}_\"\n",
    "                    for position, number in enumerate(decimals, 1)\n",
    "                ]\n",
    "        return tokens\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_tokens_to_float(tokens: List[str]) -> float:\n",
    "        \"\"\"Converts tokens representing a float value into a float.\n",
    "        NOTE: Expects that non-floating tokens are strippped off\n",
    "\n",
    "        Args:\n",
    "            tokens: List of tokens, each representing a float.\n",
    "                E.g.: ['_0_0_', '_._', '_9_-1_', '_3_-2_', '_1_-3_']\n",
    "\n",
    "        Returns:\n",
    "            float: Float representation for the list of tokens.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            float_string = \"\".join([token.split(\"_\")[1] for token in tokens])\n",
    "            float_value = float(float_string)\n",
    "        except ValueError:\n",
    "            float_value = -1\n",
    "        return float_value\n",
    "\n",
    "    \n",
    "    def convert_tokens_to_string(self, tokens: List[str]) -> str:\n",
    "        \"\"\"Converts tokens to string.\n",
    "\n",
    "        Args:\n",
    "            tokens: List of tokens.\n",
    "\n",
    "        Returns:\n",
    "            str: String representation of the tokens.\n",
    "        \"\"\"\n",
    "        return \"\".join([token.split(\"_\")[1] for token in tokens])\n",
    "        \n",
    "\n",
    "\n",
    "class Xtal2txtTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(\n",
    "        self, special_num_token:bool=False, vocab_file=None, model_max_length=None, padding_length=None, **kwargs\n",
    "    ):\n",
    "        super(Xtal2txtTokenizer, self).__init__(\n",
    "         model_max_length=model_max_length, **kwargs\n",
    "        )\n",
    "        self.truncation = False\n",
    "        self.padding = False\n",
    "        self.padding_length = padding_length\n",
    "        self.special_num_tokens = special_num_token\n",
    "        self.vocab = self.load_vocab(vocab_file)\n",
    "        self.vocab_file = vocab_file\n",
    "\n",
    "    def load_vocab(self, vocab_file):\n",
    "        _, file_extension = os.path.splitext(vocab_file)\n",
    "        if file_extension == \".txt\":\n",
    "            with open(vocab_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                vocab = file.read().splitlines()\n",
    "            return {token: idx for idx, token in enumerate(vocab)}\n",
    "        elif file_extension == \".json\":\n",
    "            with open(vocab_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                return json.load(file)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "    \n",
    "    def get_special_num_tokens(self,text):\n",
    "        num_tokenizer = NumTokenizer()\n",
    "        return num_tokenizer.num_matcher(text)\n",
    "\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        if self.special_num_tokens:\n",
    "            text = self.get_special_num_tokens(text)\n",
    "\n",
    "        tokens = list(self.vocab.keys())\n",
    "        string_tokens = [token for token in tokens if isinstance(token, str)]\n",
    "        string_tokens.sort(key=len, reverse=True)\n",
    "        escaped_tokens = [re.escape(token) for token in string_tokens]\n",
    "        pattern_str = \"|\".join(escaped_tokens)\n",
    "        pattern = re.compile(pattern_str)\n",
    "        matches = pattern.findall(text)\n",
    "\n",
    "        if self.truncation and len(matches) > self.model_max_length:\n",
    "            matches = matches[: self.model_max_length]\n",
    "\n",
    "        if self.padding and len(matches) < self.padding_length:\n",
    "            matches += [self.pad_token] * (self.padding_length - len(matches))\n",
    "\n",
    "        return matches\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def _add_tokens(self, new_tokens, **kwargs):\n",
    "        for token in new_tokens:\n",
    "            if token not in self.added_tokens_encoder:\n",
    "                self.vocab[token] = len(self.vocab)\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        return list(self.vocab.keys())[index]\n",
    "\n",
    "    def enable_truncation(self, max_length):\n",
    "        self.model_max_length = max_length\n",
    "        self.truncation = True\n",
    "\n",
    "    def disable_truncation(self):\n",
    "        self.truncation = False\n",
    "\n",
    "    def enable_padding(self, length=None):\n",
    "        self.padding = True\n",
    "        self.padding_length = length\n",
    "\n",
    "    def disable_padding(self):\n",
    "        self.padding = False\n",
    "\n",
    "    def add_special_tokens(self, special_tokens):\n",
    "        for token, value in special_tokens.items():\n",
    "            if value not in self.vocab:\n",
    "                setattr(self, token, value)\n",
    "                self.vocab[value] = len(self.vocab)\n",
    "        self.save_vocabulary(os.path.dirname(self.vocab_file))\n",
    "\n",
    "    def token_analysis(self, tokens):\n",
    "        \"\"\"This method should be implemented by the Downstream tokenizers.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save_vocabulary(self, save_directory, filename_prefix=None):\n",
    "        \"\"\"Save the vocabulary, ensures vocabularies are not overwritten. Filename follow the convention {index}-{filename_prefix}.json. Index keeps track of the latest vocabulary saved.\"\"\"\n",
    "        index = 0\n",
    "        if os.path.isdir(save_directory):\n",
    "            vocab_files = list(\n",
    "                filter(lambda x: x.endswith(\".json\"), os.listdir(save_directory))\n",
    "            )\n",
    "            for vocab_file in vocab_files:\n",
    "                try:\n",
    "                    index = max(index, int(vocab_file.split(\"-\")[0]))\n",
    "                except ValueError:\n",
    "                    pass  # Ignore files that do not start with an integer\n",
    "\n",
    "        vocab_file = os.path.join(\n",
    "            save_directory,\n",
    "            f\"{index + 1}-{filename_prefix}.json\"\n",
    "            if filename_prefix\n",
    "            else f\"{index + 1}.json\",\n",
    "        )\n",
    "\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.vocab, f, ensure_ascii=False)\n",
    "\n",
    "        return (vocab_file,)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n",
    "        if pretrained_model_name_or_path is not None:\n",
    "            if os.path.isdir(pretrained_model_name_or_path):\n",
    "                vocab_files = list(\n",
    "                    filter(\n",
    "                        lambda x: x.endswith(\".json\"),\n",
    "                        os.listdir(pretrained_model_name_or_path),\n",
    "                    )\n",
    "                )\n",
    "                vocab_files.sort(key=lambda x: int(x.split(\"-\")[0]))\n",
    "                vocab_file = os.path.join(\n",
    "                    pretrained_model_name_or_path, vocab_files[-1]\n",
    "                )\n",
    "\n",
    "        if vocab_file is None:\n",
    "            raise ValueError(\"You should specify a path to a vocab file\")\n",
    "\n",
    "        with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            vocab = json.load(f)\n",
    "\n",
    "        tokenizer = cls(vocab_file, *inputs, **kwargs)\n",
    "        tokenizer.vocab = vocab\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "\n",
    "class SliceTokenizer(Xtal2txtTokenizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        special_num_token:bool=False,\n",
    "        vocab_file=None,\n",
    "        model_max_length=None,\n",
    "        padding_length=None,\n",
    "        **kwargs,\n",
    "    ):  \n",
    "        if special_num_token:\n",
    "            vocab_file = SLICE_RT_VOCAB if vocab_file is None else vocab_file\n",
    "        else:\n",
    "            vocab_file = SLICE_VOCAB if vocab_file is None else vocab_file\n",
    "        super(SliceTokenizer, self).__init__(\n",
    "            special_num_token=special_num_token,\n",
    "            vocab_file=vocab_file,\n",
    "            model_max_length=model_max_length,\n",
    "            padding_length=padding_length,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    \n",
    "\n",
    "class CifTokenizer(Xtal2txtTokenizer):\n",
    "    def __init__(\n",
    "        self, special_num_token:bool = False, vocab_file=None, model_max_length=None, padding_length=None, **kwargs\n",
    "    ):\n",
    "        if special_num_token:\n",
    "            vocab_file = CIF_RT_VOCAB \n",
    "        else:\n",
    "            vocab_file = CIF_VOCAB \n",
    "        super(CifTokenizer, self).__init__(\n",
    "            special_num_token=special_num_token,\n",
    "            vocab_file=vocab_file,\n",
    "            model_max_length=model_max_length,\n",
    "            padding_length=padding_length,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def token_analysis(self, list_of_tokens):\n",
    "        \"\"\"Takes tokens after tokenize and returns a list with replacing the tokens with their MASK token. The\n",
    "        token type is determined from the dict declared globally, and the token is replaced with the corresponding MASK token.\"\"\"\n",
    "        analysis_masks = ANALYSIS_MASK_TOKENS\n",
    "        token_type = CIF_ANALYSIS_DICT\n",
    "        return [\n",
    "            analysis_masks[next((k for k, v in token_type.items() if token in v), None)]\n",
    "            for token in list_of_tokens\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tokenize_(text: str) -> List[str]:\n",
    "    \"\"\"Tokenization of numbers as in RT.\n",
    "        '0.9' -> '_0_0_', '_._', '_9_-1_'\n",
    "\n",
    "    Args:\n",
    "        text: number as string to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        extracted tokens.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    regex = re.compile(r\"(\\+|-)?(\\d+)(\\.)?(\\d+)?\\s*\")\n",
    "    matched = regex.match(text)\n",
    "    if matched:\n",
    "        sign, units, dot, decimals = matched.groups()\n",
    "        tokens = []\n",
    "        if sign:\n",
    "            tokens += [f\"_{sign}_\"]\n",
    "        tokens += [\n",
    "            f\"_{number}_{position}_\" for position, number in enumerate(units[::-1])\n",
    "        ][::-1]\n",
    "        if dot:\n",
    "            tokens += [f\"_{dot}_\"]\n",
    "        if decimals:\n",
    "            tokens += [\n",
    "                f\"_{number}_-{position}_\"\n",
    "                for position, number in enumerate(decimals, 1)\n",
    "            ]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_matcher_( text: str) -> str:\n",
    "    \"\"\"Extract numbers from a sentence and replace them with tokens.\"\"\"\n",
    "    pattern = r\"\\d+(?:\\.\\d+)?\"  # Match any number, whether it is part of a string or not\n",
    "    matches = list(re.finditer(pattern, text))\n",
    "    for match in reversed(matches):\n",
    "        start, end = match.start(), match.end()\n",
    "        tokens = tokenize_(match.group())\n",
    "        replacement = ''.join(tokens)\n",
    "        text = text[:start] + replacement + text[end:]\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'InCuS_2_1__3_0_Hi'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_matcher_(\"InCuS23Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_matcher_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"InCuS23Hi\"\n",
    "s[5:7]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciftokenizer = CifTokenizer(special_num_token=True,model_max_length=512, truncation=False, padding=False)\n",
    "\n",
    "slicetokenizer = SliceTokenizer(special_num_token=True,model_max_length=512, truncation=False, padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"data_InCuS2\\n_symmetry_space_group_name_H-M   P1\\n_cell_length_a   5.52\\n_cell_length_b   5.52\\n_cell_length_c   6.796\\n_cell_angle_alpha   113.963\\n_cell_angle_beta   113.963\\n_cell_angle_gamma   90.0\\n_symmetry_Int_Tables_number   1\\n_chemical_formula_structural   InCuS2\\n_chemical_formula_sum   'In2 Cu2 S4'\\n_cell_volume   169.534\\n_cell_formula_units_Z   2\\nloop_\\n _symmetry_equiv_pos_site_id\\n _symmetry_equiv_pos_as_xyz\\n  1  'x, y, z'\\nloop_\\n _atom_type_symbol\\n _atom_type_oxidation_number\\n  In3+  3.0\\n  Cu+  1.0\\n  S2-  -2.0\\nloop_\\n _atom_site_type_symbol\\n _atom_site_label\\n _atom_site_symmetry_multiplicity\\n _atom_site_fract_x\\n _atom_site_fract_y\\n _atom_site_fract_z\\n _atom_site_occupancy\\n  In3+  In0  1  0.609  0.603  0.097  1.0\\n  In3+  In1  1  0.854  0.341  0.594  1.0\\n  Cu+  Cu2  1  0.36  0.347  0.348  1.0\\n  Cu+  Cu3  1  0.361  0.853  0.609  1.0\\n  S2-  S4  1  0.001  0.963  0.342  1.0\\n  S2-  S5  1  0.218  0.749  0.839  1.0\\n  S2-  S6  1  0.445  0.471  0.358  1.0\\n  S2-  S7  1  0.721  0.188  0.853  1.0\\n\""
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"data_InCuS_2_0_\\n_symmetry_space_group_name_H-M_1_0_  P1\\n_cell_len_5_0__.__5_-1__2_-2_a _5_0__.__5_-1__2_-2_52_6_0__.__7_-1__9_-2__6_-3__1_2__1_1__3_0__.__9_-1__6__1_2__1_1__3_0__.__9_-1__6_-_9_1__0_0__.__0_-1__-3_3_h_b   5.52_1_0__cell_length_c   6.796\\n_cell_angl_2_0__alpha   113.963\\n_cell_a_2_0_2__4_0__gle_beta   1_1_2__6_1__9_0__.__5_-1__3_-2__4_2_0_-3__cell_angle_gamma   90.0\\n_symmetry_Int_Tables_number   1\\n_c_1_0_emical_formula_structural   InCuS2\\n_chemical_formula_sum   'In2 Cu_3_0_3_0__.__0__1_0__._2_0__2_0__.__0_-1_-1_4'\\n_cell_volume   169.534\\n_cell_formula_units_Z   2\\nloop_\\n _symmetry_equiv_pos_site_id\\n _symmetry_equiv_pos_as_xyz\\n  1  'x, y, z'\\nloop_\\n _atom_type_symbol\\n_3_0___0__1__0_0__._0_0__._0_0__._1_0__.__3_0_0_1__1__0_0__._0_0__._0_0__._1_0__.__0_-1_2__1__0_0___0_0__._0_0__._1_0__.__0_-1_3__1__0_0__._0_0__._0_0__._1_0__._2_0__4__1__0_0__._0_0__._0_0__._1_0__._2_0__5__1__0_0__._0_0__._0_0__._1_0__._2_0__6__1__0_0__._0_0__._0_0__._1_0__._2_0__7__1__0_0__._0_0__._0_0__._1_0__.__0_-1__-1__5_-2__3_-3_1__8_-2__8_-3_1__2_-2__1_-3_-1__-1__5_-2__8_-3_1__7_-2__1_-3_1__4_-2__5_-3_-1__-1__3_-2__9_-3_1__4_-2__9_-3_1__1_-2__8_-3_-1__-1__4_-2__2_-3_1__6_-2__3_-3_1__0_-2__1_-3_-1__-1__0_-2__9_-3_1__5_-2__3_-3_1__6_-2__1_-3_1__4_-2__8_-3_1__4_-2__7_-3_-1__6_-2_-1__9_-2__4_-3_1__4_-2__1_-3_1__5_-2__4_-3___-1__9_-2__7_-3_1__0_-2__3_-3_1__0_-2__9_-3_m_type_oxidation_number\\n  In3+  3.0\\n  Cu+  1.0\\n  S2-  -2.0\\nloop_\\n _atom_site_type_symbol\\n _atom_site_label\\n _atom_site_symmetry_multiplicity\\n _atom_site_fract_x\\n _atom_site_fract_y\\n _atom_site_fract_z\\n _atom_site_occupancy\\n  In3+  In0  1  0.609  0.603  0.097  1.0\\n  In3+  In1  1  0.854  0.341  0.594  1.0\\n  Cu+  Cu2  1  0.36  0.347  0.348  1.0\\n  Cu+  Cu3  1  0.361  0.853  0.609  1.0\\n  S2-  S4  1  0.001  0.963  0.342  1.0\\n  S2-  S5  1  0.218  0.749  0.839  1.0\\n  S2-  S6  1  0.445  0.471  0.358  1.0\\n  S2-  S7  1  0.721  0.188  0.853  1.0\\n\""
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokenizer.num_matcher(cif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'InCuS_2_0_h Hi Cu_2_0_ _2_0_ _0_0__.__2_-1_ '"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokenizer = NumTokenizer_()\n",
    "num_tokenizer.num_matcher(\"InCuS2h Hi Cu2 2 0.2 \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'InCuS2h Hi Cu2 _2_0_ 0._2_0_ '"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NumTokenizer().num_matcher(\"InCuS2h Hi Cu2 2 0.2 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '2', '2', '0.2']\n",
      "None\n",
      "InCuS2h Hi Cu2 _2_0_ 0._2_0_ \n"
     ]
    }
   ],
   "source": [
    "def hi(text: str) -> str:\n",
    "        \"\"\"Extract numbers from a sentence and replace them with tokens.\"\"\"\n",
    "        matches = re.findall(r'\\d+(?:\\.\\d+)?', text)\n",
    "        print(matches)\n",
    "\n",
    "num_tokenizer = NumTokenizer_()\n",
    "print(hi(\"InCuS2h Hi Cu2 2 0.2 \"))\n",
    "print(num_tokenizer.num_matcher(\"InCuS2h Hi Cu2 2 0.2 \"))\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Cu',\n",
       " 'S',\n",
       " ' ',\n",
       " 'H',\n",
       " ' ',\n",
       " '_2_0_',\n",
       " '.',\n",
       " ' ',\n",
       " '_5_0_',\n",
       " '_._',\n",
       " '_2_-1_',\n",
       " '  ']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ciftokenizer.tokenize(\"InCuS2h Hi 2.0 5.2  h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\b(\\d+(?:\\.\\d*)?|\\d+\\w+)\\b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2S', '2', '3', '5.0', '123', '2']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def find_numbers(text):\n",
    "  \"\"\"\n",
    "  This function finds standalone numbers and numbers within a single letter in a text string.\n",
    "\n",
    "  Args:\n",
    "      text: The text string to search.\n",
    "\n",
    "  Returns:\n",
    "      A list of matched numbers.\n",
    "  \"\"\"\n",
    "  pattern = r\"[a-zA-Z]?(\\d+(?:\\.\\d*)?|\\d[a-zA-Z]?)\\b\"  # Match digit followed by single letter\n",
    "  \n",
    "  matches = re.findall(pattern, text)\n",
    "  return matches\n",
    "\n",
    "# Example usage\n",
    "text = \"In2S, Cu2, He3, 5.0, 123, FeO2\"\n",
    "numbers = find_numbers(text)\n",
    "print(numbers)  # Output: ['2', '2', '5.0', '123']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5.0', '123']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def find_numbers(text):\n",
    "    \"\"\"\n",
    "    This function finds standalone numbers and numbers within a single letter in a text string.\n",
    "\n",
    "    Args:\n",
    "        text: The text string to search.\n",
    "\n",
    "    Returns:\n",
    "        A list of matched numbers.\n",
    "    \"\"\"\n",
    "    pattern = r\"(?<!\\w)(?:(?<=\\D)\\d+(?:\\.\\d+)?|\\d+(?=\\D))(?!\\w)\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches\n",
    "\n",
    "# Example usage\n",
    "text = \"In2S, Cu2, He3, 5.0, 123, FeO2, InCuS2h\"\n",
    "numbers = find_numbers(text)\n",
    "print(numbers)  # Output: ['2', '2', '3', '5.0', '123', '2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.NumTokenizer_"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NumTokenizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '2', '3', '5.0', '123', '2', '2', '2', '3', '0.2', '5.8']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def find_numbers_in_strings(text):\n",
    "    \"\"\"\n",
    "    This function finds numbers that are part of a string and standalone numbers.\n",
    "\n",
    "    Args:\n",
    "        text: The text string to search.\n",
    "\n",
    "    Returns:\n",
    "        A list of matched numbers.\n",
    "    \"\"\"\n",
    "    pattern = r\"\\d+(?:\\.\\d+)?\"  # Match any number, whether it is part of a string or not\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches\n",
    "\n",
    "# Example usage\n",
    "text = \"In2S, Cu2, He3, 5.0, 123, FeO2, InCuS2h, Fe2So3, 0.2, 5.8\"\n",
    "numbers = find_numbers_in_strings(text)\n",
    "print(numbers)  # Output: ['2', '2', '3', '5', '0', '123', '2', '2']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/so87pot/n0w0f/xtal2txt/src/xtal2txt/vocabs/crystal_llm_vocab_rt.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtal2txt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
